# What is Huffman Coding?

> 原文来自于[BaseClass - What is Huffman Coding?](https://www.baseclass.io/huffman-coding/)

The Huffman Coding algorithm is a building block of many *compression* algorithms, such as DEFLATE - which is used by the PNG image format and GZIP.

> **compression**	n. 压缩，浓缩；压榨，压迫

## Why should I care?

Have you ever wanted to know:

- How do we compress something, without losing any data?
- Why do some things compress better than others?
- How does GZIP work?

## In 5 minutes or less:

Suppose we want to compress a string (Huffman coding can be used with any data, but strings makes good examples).

Inevitably, some characters will appear more often than others in the text to be compressed. Huffman Coding *takes advantage of* that fact, and encodes the text so that **the most used characters take up less space than the less common ones**.

> **take advantage of sb/sth** :
>
> + **to use the good things in a situation**, 利用；利用…的机会
>
>   *I thought I'd take advantage of the sports facilities [设施，设备] while I'm here.*
>
> + **to treat someone badly in order to get something good from them**, 占…的便宜；欺骗
>
>   *I know she's offered to babysit[(孩子家长外出时)临时代为照看婴孩], but I don't want her to think we're taking advantage of her.*

## Encoding a string

Let’s use Huffman Encoding to compress a (*partial*) quote from Yoda; “do or do not”.

> **partial**	adj. 局部的；偏爱的；不公平的

“do or do not” is 12 characters long. It has a few *duplicate* characters, so it should compress quite nicely.

> **duplicate**	adj. 复制的；二重的；vt. 复制；使加倍

For the *sake* of argument, we’ll assume that storing each character takes 8 bits (Character Encoding is another topic entirely). This sentence would cost us 96 bits, but we can do better with Huffman coding!

> **sake** 	n. 目的；利益；理由；日本米酒

We begin by building a tree structure. The most common characters in our data will be closer to the root of the tree, while the nodes furthest away from the root represent the less common characters.

Here’s the Huffman Tree for the string “do or do not”:

![Huffman Tree](https://cdn.jsdelivr.net/gh/mahoo12138/js-css-cdn/note-images/20210308162558.png)

The most common characters in our string are ‘o’s (4 *occurrences*) and spaces (3 occurrences). Notice that the path to those characters is only two steps away from the root, compared to three for the least common character (’t’).

> **occurrence**	n. 发生；出现；事件；发现
>
> + something that happens, 发生的事；事件；遭遇
>
>   *Street-fights are an everyday occurrence in this area of the city.* 在该市的这一地区，街斗每天都会发生。
>
> + the fact of something existing, or how much of it exists, 存在；存在的数量
>
>   *The study compares the occurrence of heart disease in various countries.*

Now, instead of storing the character itself, we can store the **path to the character** instead.

We start at the root node and work our way down the tree towards the character we want to encode. We’ll store a `0` if we take the left-hand path, and a `1` if we take the right.

Here’s how we would encode the first character, `d`, using this tree:

![Huffman Tree encoding 'd'](https://cdn.jsdelivr.net/gh/mahoo12138/js-css-cdn/note-images/20210308162611.png)

The end result is `1` `0` `0` - 3 bits instead of 8. That’s quite an *improvement*!

> *improvement*	n. 改进，改善；提高

The entire encoded string looks like this:

!['do or do not' encoded using the tree](https://cdn.jsdelivr.net/gh/mahoo12138/js-css-cdn/note-images/20210308162613.png)

This is 29 bits instead of 96, with no data loss. Great!

## Decoding our string

To decode our text, we simply follow each `0` (left branch) or `1` (right branch) until we reach a character. We write that character down, and then start again from the top:

![Decoding using the tree'](https://cdn.jsdelivr.net/gh/mahoo12138/js-css-cdn/note-images/20210308162619.png)

## Sending the encoded text

But wait.. when we send the encoded text to somebody else, don’t they need the tree too? Yes! **The other side needs the same Huffman tree in order to decode the text correctly**.

The simplest, but least efficient way, is to simply send the tree along with the compressed text.

We could also agree on a tree first, and both use that tree when encoding or decoding any string. That works OK when we can predict the distribution of characters *ahead of time*, and could build a relatively efficient tree without seeing the specific thing we’re encoding first (as we could with English text, for example).

> **ahead of time**: earlier than a particular moment, 提前
>
> *Let's meet for lunch. I'll call you ahead of time to decide exactly when and where.*

Another option is to send just enough information to allow the other side to build the same tree as us (this is how GZIP works). We could send the total number of times that each character occurs, for example. We have to be careful though; **there is more than one possible Huffman tree for the same block of text**, so we have to make sure we both construct the tree in **exactly** the same way.

